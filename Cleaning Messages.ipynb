{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T06:50:52.931139Z",
     "start_time": "2019-02-04T06:50:51.521554Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "google_vec_file = '~/Downloads/GoogleNews-vectors-negative300.bin'\n",
    "google_model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_whole = pd.concat([messages1['message'],messages2['message'],messages3['message']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4898778072007074"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_df(messages):\n",
    "    text = []\n",
    "    button = []\n",
    "    for entry in messages:\n",
    "        try:\n",
    "            text.append(entry['message']['text'])\n",
    "        except:\n",
    "            button.append(entry)\n",
    "    print(len(text)/len(button))\n",
    "    return pd.DataFrame(text,columns=['text'])\n",
    "\n",
    "text = text_df(messages_whole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: 1/3 of interactions were text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T08:32:58.062986Z",
     "start_time": "2019-02-04T08:32:58.039242Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_messages(messages):\n",
    "    messs = []\n",
    "    for i in messages['message']:\n",
    "        try:\n",
    "            messs.append(i['message']['text'])\n",
    "        except:\n",
    "            pass\n",
    "    messages['messages'] = messs\n",
    "    \n",
    "    recip = []\n",
    "    for i in messages['message']:\n",
    "        try:\n",
    "            recip.append(i['recipient']['id'])\n",
    "        except:\n",
    "            pass\n",
    "    messages['recipient'] = recip\n",
    "    messages['recipient'] = messages['recipient'].astype('str')\n",
    "    \n",
    "    send = []\n",
    "    for i in messages['message']:\n",
    "        try:\n",
    "            send.append(i['sender']['id'])\n",
    "        except:\n",
    "            pass\n",
    "    messages['sender'] = send\n",
    "    messages['sender'] = messages['sender'].astype('str')\n",
    "    \n",
    "    messages['rp_page'] = messages['recipient']\n",
    "    messages['ru_page'] = messages['recipient']\n",
    "    messages['sp_page'] = messages['sender']\n",
    "    messages['su_page'] = messages['sender']\n",
    "    \n",
    "    messages['rp_follow'] = messages['recipient']\n",
    "    messages['ru_follow'] = messages['recipient']\n",
    "    messages['sp_follow'] = messages['sender']\n",
    "    messages['su_follow'] = messages['sender']\n",
    "    \n",
    "    messages = messages.rename(index=str, columns={\"date\": \"datemess\"})\n",
    "    \n",
    "    return messages.drop(columns=['_id', '__v', 'message','senderId'])\n",
    "\n",
    "messages = clean_messages(messages_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T08:37:34.766175Z",
     "start_time": "2019-02-04T08:37:34.723548Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_followers(folls):\n",
    "    folls = folls.drop(columns=['__v','attributes','_id','subscriptionType'])\n",
    "    folls = folls.set_index(['profile'])\n",
    "    folls = folls.filter(like=\"'gender':\",axis=0)\n",
    "    folls = folls.filter(like=\"'timezone':\",axis=0)\n",
    "    folls = folls.filter(like=\"'locale':\",axis=0)\n",
    "    folls = folls.reset_index()\n",
    "    \n",
    "    gender = []\n",
    "    for i in folls['profile']:\n",
    "        try:\n",
    "            gender.append(i['gender'])\n",
    "        except:\n",
    "            pass\n",
    "    folls['gender'] = gender\n",
    "    \n",
    "    timezone = []\n",
    "    for i in folls['profile']:\n",
    "        try:\n",
    "            timezone.append(i['timezone'])\n",
    "        except:\n",
    "            pass\n",
    "    folls['timezone'] = timezone\n",
    "    \n",
    "    locale = []\n",
    "    for i in folls['profile']:\n",
    "        try:\n",
    "            locale.append(i['locale'])\n",
    "        except:\n",
    "            pass\n",
    "    folls['locale'] = locale\n",
    "    \n",
    "    folls['pageId'] = folls['pageId'].astype('str')\n",
    "    folls['userId'] = folls['userId'].astype('str')\n",
    "    \n",
    "    folls['rp_follow'] = folls['pageId']\n",
    "    folls['ru_follow'] = folls['userId']\n",
    "    folls['sp_follow'] = folls['pageId']\n",
    "    folls['su_follow'] = folls['userId']\n",
    "    \n",
    "    return folls.drop(columns=['profile'])\n",
    "\n",
    "followers = clean_followers(folls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T08:40:57.782815Z",
     "start_time": "2019-02-04T08:40:57.752758Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_pages(page):\n",
    "    \n",
    "    page = page.drop(columns=['__v','_id','accessToken','administrators','botId','fetchedConversations','insufficientPermissions','insufficientPermissionsLastChecked','tags','whitelistedDomains'])\n",
    "    \n",
    "    fans = []\n",
    "    for i in page['information']:\n",
    "        try:\n",
    "            fans.append(i['fanCount'])\n",
    "        except:\n",
    "            fans.append(\"NaN\")\n",
    "    page['fans'] = fans\n",
    "\n",
    "    pagename = []\n",
    "    for i in page['information']:\n",
    "        try:\n",
    "            pagename.append(i['username'])\n",
    "        except:\n",
    "            pagename.append(\"NaN\")\n",
    "    page['pagename'] = pagename\n",
    "    page = page.drop(columns=['information'])\n",
    "    \n",
    "    for i in page['pagename']:\n",
    "        if i == \"NaN\":\n",
    "            page['pagename'] = page['name']\n",
    "    page = page.drop(columns=['name'])\n",
    "    \n",
    "    page['pageId'] = page['pageId'].astype('str')\n",
    "    page['userId'] = page['userId'].astype('str')\n",
    "    \n",
    "    page['rp_page'] = page['pageId']\n",
    "    page['ru_page'] = page['userId']\n",
    "    page['sp_page'] = page['pageId']\n",
    "    page['su_page'] = page['userId']\n",
    "    \n",
    "    page = page.rename(index=str, columns={\"date\": \"datepage\"})\n",
    "    \n",
    "    return page\n",
    "\n",
    "pages = clean_pages(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "messpage = pd.merge(messages, pages, how='inner', on=['rp_page'])\n",
    "\n",
    "df_f = pd.merge(messpage, folls, how='inner', on=['su_follow'])\n",
    "\n",
    "df_final = df_f.drop(columns=['pageId_x','recipient','sender','ru_page_x','sp_page_x','su_page_x','rp_follow_x',\n",
    "               'pageId','userId_y','rp_follow_y','ru_follow_y','sp_follow_y','ru_follow_x','sp_follow_x',\n",
    "                   'pageId_y','userId_x','ru_page_y','sp_page_y','su_page_y'])\n",
    "\n",
    "df_final.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store data in pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mergedmess.pkl','wb') as file:\n",
    "    pickle.dump(df_final,file)\n",
    "    \n",
    "# with open('mergedmess.pkl','rb') as file:\n",
    "#     mergedmess = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
